data:
  task_name: harmful
  split: test_public # val, test_public, test_private
  train_fraction: 0.4 # temp, debug # if split=val, then have to =0.5
  shuffle_seed: 0
  behavior_ids: []
  # sample_ids: "1-1,1-2,1-3,1-4,10-1,10-2,10-3,10-4" # list of example IDs to select specific samples
  detailed_behaviors: 
  hint_included: 
  n_irrelevant_tools: 0

models:
  lm_name: openai/gpt-4o-mini # gpt-4o-mini, gpt-5-mini, gemini-2.5-flash, qwen3-8b 
  lm_temperature: 0 # 0 for others
  seed: 42
  max_tokens: 4096 # 4096 for others
  refusal_judge_model:  openai/gpt-4o # openai/gpt-4o-mini or openai/gpt-4o-2024-08-06
  semantic_judge_model: openai/gpt-4o # openai/gpt-4o-mini or openai/gpt-4o-2024-08-06
  # api_base: 
  api_base: https://api.shubiaobiao.cn/v1/
  # headers:
    # x-foo: "true"

optimization:
  optim_seed: 6793115
  auto_mode: medium # light, medium, heavy
  num_threads: 4
  algorithm: mipro # one of: mipro, copro, gepa
  run_optimization: true
  optimize_refusal: false  # set true to optimize using refusal metric instead of task success

experiment:
  name: agentharm
  uri: http://127.0.0.1:5000
